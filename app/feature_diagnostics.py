"""
ç‰¹å¾´é‡è¨ºæ–­ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å¤šé‡å…±ç·šæ€§ã€ãƒ‡ãƒ¼ã‚¿å“è³ªã€ç‰¹å¾´é‡ç›¸é–¢ã‚’åˆ†æ

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ Data Scientist ãŒç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®
å“è³ªã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ç¾¤ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, List, Optional
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from app import queries


class FeatureDiagnostics:
    """ç‰¹å¾´é‡è¨ºæ–­ã‚¨ãƒ³ã‚¸ãƒ³"""

    @staticmethod
    def calculate_vif(X: np.ndarray, feature_names: List[str]) -> pd.DataFrame:
        """
        VIFï¼ˆVariance Inflation Factorï¼‰ã‚’è¨ˆç®—

        VIF > 5 ã®ç‰¹å¾´é‡ã¯é«˜ã„å¤šé‡å…±ç·šæ€§ã‚’ç¤ºã™
        VIF > 10 ã®ç‰¹å¾´é‡ã¯å‰Šé™¤æ¨å¥¨

        Args:
            X: ç‰¹å¾´é‡è¡Œåˆ— (n_samples, n_features)
            feature_names: ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆ

        Returns:
            VIF ã‚¹ã‚³ã‚¢ DataFrame
        """
        try:
            from statsmodels.stats.outliers_influence import variance_inflation_factor
        except ImportError:
            return pd.DataFrame(
                {
                    "Feature": feature_names,
                    "VIF": [np.nan] * len(feature_names),
                    "Status": ["statsmodels not installed"] * len(feature_names),
                }
            )

        # NaN ã¨ inf ã‚’å‡¦ç†
        X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

        vif_data = []
        for i in range(X_clean.shape[1]):
            try:
                vif = variance_inflation_factor(X_clean, i)
                status = self._vif_status(vif)
                vif_data.append({"Feature": feature_names[i], "VIF": vif, "Status": status})
            except Exception as e:
                vif_data.append(
                    {"Feature": feature_names[i], "VIF": np.nan, "Status": f"Error: {str(e)[:30]}"}
                )

        return pd.DataFrame(vif_data).sort_values("VIF", ascending=False)

    @staticmethod
    def _vif_status(vif: float) -> str:
        """VIF ã‚¹ã‚³ã‚¢ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’åˆ¤å®š"""
        if np.isnan(vif) or np.isinf(vif):
            return "âš ï¸ è¨ˆç®—ä¸å¯"
        elif vif > 10:
            return "ğŸ”´ å‰Šé™¤æ¨å¥¨ (VIF > 10)"
        elif vif > 5:
            return "ğŸŸ¡ ç¢ºèªæ¨å¥¨ (VIF > 5)"
        elif vif > 2:
            return "ğŸŸ¢ æ³¨æ„ (VIF > 2)"
        else:
            return "âœ… è‰¯å¥½ (VIF <= 2)"

    @staticmethod
    def calculate_correlation_matrix(X: np.ndarray, feature_names: List[str]) -> pd.DataFrame:
        """
        ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—

        ç›¸é–¢ä¿‚æ•° > 0.8 ã®ç‰¹å¾´é‡ãƒšã‚¢ã¯é«˜åº¦ã«ç›¸é–¢ã—ã¦ã„ã‚‹

        Args:
            X: ç‰¹å¾´é‡è¡Œåˆ—
            feature_names: ç‰¹å¾´é‡å

        Returns:
            ç›¸é–¢è¡Œåˆ— DataFrame
        """
        X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

        corr_matrix = np.corrcoef(X_clean.T)

        return pd.DataFrame(corr_matrix, index=feature_names, columns=feature_names)

    @staticmethod
    def find_highly_correlated_pairs(
        X: np.ndarray, feature_names: List[str], threshold: float = 0.8
    ) -> List[Dict]:
        """
        é«˜åº¦ã«ç›¸é–¢ã—ãŸç‰¹å¾´é‡ãƒšã‚¢ã‚’æ¤œå‡º

        Args:
            X: ç‰¹å¾´é‡è¡Œåˆ—
            feature_names: ç‰¹å¾´é‡å
            threshold: ç›¸é–¢ä¿‚æ•°ã®é–¾å€¤ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 0.8)

        Returns:
            ç›¸é–¢ãƒšã‚¢æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        corr_matrix = FeatureDiagnostics.calculate_correlation_matrix(X, feature_names)

        pairs = []
        for i in range(len(feature_names)):
            for j in range(i + 1, len(feature_names)):
                corr = abs(corr_matrix.iloc[i, j])
                if corr > threshold:
                    pairs.append(
                        {
                            "Feature 1": feature_names[i],
                            "Feature 2": feature_names[j],
                            "Correlation": corr,
                            "Recommendation": self._correlation_recommendation(corr),
                        }
                    )

        return sorted(pairs, key=lambda x: x["Correlation"], reverse=True)

    @staticmethod
    def _correlation_recommendation(corr: float) -> str:
        """ç›¸é–¢ä¿‚æ•°ã‹ã‚‰ã®æ¨å¥¨ã‚’ç”Ÿæˆ"""
        if corr > 0.95:
            return "ğŸ”´ ã©ã¡ã‚‰ã‹å‰Šé™¤"
        elif corr > 0.85:
            return "ğŸŸ¡ é–¢é€£æ€§ç¢ºèªå¾Œã€ç‰‡æ–¹å‰Šé™¤ã‚’æ¤œè¨"
        else:
            return "âš ï¸ ç›£è¦–"

    @staticmethod
    def check_feature_variance(X: np.ndarray, feature_names: List[str]) -> pd.DataFrame:
        """
        å„ç‰¹å¾´é‡ã®åˆ†æ•£ã‚’ç¢ºèª
        åˆ†æ•£ãŒæ¥µç«¯ã«å°ã•ã„ç‰¹å¾´é‡ã¯äºˆæ¸¬åŠ›ãŒå¼±ã„

        Args:
            X: ç‰¹å¾´é‡è¡Œåˆ—
            feature_names: ç‰¹å¾´é‡å

        Returns:
            åˆ†æ•£æƒ…å ± DataFrame
        """
        X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

        variances = np.var(X_clean, axis=0)
        means = np.mean(X_clean, axis=0)

        # Coefficient of Variation
        cv = np.where(means != 0, variances / (means**2), 0)

        variance_data = []
        for i, feature_name in enumerate(feature_names):
            variance_data.append(
                {
                    "Feature": feature_name,
                    "Variance": variances[i],
                    "Mean": means[i],
                    "Std": np.sqrt(variances[i]),
                    "CV": cv[i],
                    "Status": self._variance_status(variances[i]),
                }
            )

        return pd.DataFrame(variance_data).sort_values("Variance", ascending=False)

    @staticmethod
    def _variance_status(variance: float) -> str:
        """åˆ†æ•£ã‹ã‚‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’åˆ¤å®š"""
        if variance < 0.001:
            return "ğŸ”´ ã»ã¼å®šæ•° (å‰Šé™¤æ¨å¥¨)"
        elif variance < 0.01:
            return "ğŸŸ¡ ä½åˆ†æ•£"
        else:
            return "âœ… æ­£å¸¸"

    @staticmethod
    def check_missing_values(X: np.ndarray, feature_names: List[str]) -> pd.DataFrame:
        """
        å„ç‰¹å¾´é‡ã®æ¬ æå€¤ã‚’ç¢ºèª

        Args:
            X: ç‰¹å¾´é‡è¡Œåˆ—ï¼ˆNaN ã‚’å«ã‚€å¯èƒ½æ€§ã‚ã‚Šï¼‰
            feature_names: ç‰¹å¾´é‡å

        Returns:
            æ¬ ææƒ…å ± DataFrame
        """
        missing_counts = np.sum(np.isnan(X), axis=0)
        total_samples = X.shape[0]
        missing_pct = (missing_counts / total_samples) * 100

        missing_data = []
        for i, feature_name in enumerate(feature_names):
            missing_data.append(
                {
                    "Feature": feature_name,
                    "Missing Count": int(missing_counts[i]),
                    "Missing %": missing_pct[i],
                    "Status": self._missing_status(missing_pct[i]),
                }
            )

        return pd.DataFrame(missing_data).sort_values("Missing %", ascending=False)

    @staticmethod
    def _missing_status(missing_pct: float) -> str:
        """æ¬ æç‡ã‹ã‚‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’åˆ¤å®š"""
        if missing_pct > 30:
            return "ğŸ”´ å‰Šé™¤æ¨å¥¨"
        elif missing_pct > 10:
            return "ğŸŸ¡ è£œå®Œæ–¹æ³•ã‚’æ¤œè¨"
        else:
            return "âœ… è¨±å®¹ç¯„å›²"

    @staticmethod
    def generate_diagnostics_report(
        X: np.ndarray, feature_names: List[str], output_path: Optional[Path] = None
    ) -> Dict:
        """
        åŒ…æ‹¬çš„ãªè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            X: ç‰¹å¾´é‡è¡Œåˆ—
            feature_names: ç‰¹å¾´é‡å
            output_path: ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å…ˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è¨ºæ–­çµæœã®è¾æ›¸
        """
        report = {
            "vif_analysis": FeatureDiagnostics.calculate_vif(X, feature_names),
            "correlation_pairs": FeatureDiagnostics.find_highly_correlated_pairs(X, feature_names),
            "variance_analysis": FeatureDiagnostics.check_feature_variance(X, feature_names),
            "missing_analysis": FeatureDiagnostics.check_missing_values(X, feature_names),
            "summary": {
                "total_features": len(feature_names),
                "high_vif_features": len(
                    [
                        f
                        for f in report.get("vif_analysis", [])
                        if "VIF > 10" in str(f.get("Status", ""))
                    ]
                ),
                "high_corr_pairs": len(report.get("correlation_pairs", [])),
            },
        }

        if output_path:
            import json

            with open(output_path, "w") as f:
                json.dump(
                    {k: v for k, v in report.items() if k != "summary"}, f, indent=2, default=str
                )

        return report


# ç°¡ç•¥ç‰ˆã®è¨ºæ–­é–¢æ•°ï¼ˆStreamlit UI ç”¨ï¼‰
def diagnose_features_simple(X: np.ndarray, feature_names: List[str]) -> Dict:
    """
    ç°¡ç•¥è¨ºæ–­ï¼ˆé‡ã„è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    Streamlit UI ã§ã®ä½¿ç”¨ã‚’æƒ³å®š
    """
    diag = FeatureDiagnostics()

    return {
        "variance": diag.check_feature_variance(X, feature_names),
        "missing": diag.check_missing_values(X, feature_names),
        "correlations": diag.find_highly_correlated_pairs(X, feature_names, threshold=0.85),
    }
