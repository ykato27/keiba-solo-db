"""
Data Scientist æ”¹å–„ã®çµ±åˆãƒ†ã‚¹ãƒˆ

Feature Diagnostics ã¨ Model Training Enhanced ã®
æ©Ÿèƒ½ã‚’çµ±åˆçš„ã«ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

å®Ÿè¡Œæ–¹æ³•:
    python test_ds_improvements.py
"""

import sys
from pathlib import Path
import numpy as np

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent))

from app import features as feat_module
from app.feature_diagnostics import FeatureDiagnostics, diagnose_features_simple
from app.test_data import generate_test_horses, generate_test_entries, generate_test_races
from app import queries, db as app_db
from app.model_training_enhanced import ModelTrainingEnhanced


def test_feature_diagnostics():
    """ç‰¹å¾´é‡è¨ºæ–­ã®ãƒ†ã‚¹ãƒˆ"""
    print("=" * 80)
    print("ğŸ§ª ãƒ†ã‚¹ãƒˆ 1: ç‰¹å¾´é‡è¨ºæ–­ (VIF, ç›¸é–¢, åˆ†æ•£)")
    print("=" * 80)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    print("\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    conn = app_db.get_connection()
    races = generate_test_races(years=1)
    for race in races[:100]:  # æœ€åˆã®100ãƒ¬ãƒ¼ã‚¹
        conn.execute("""
            INSERT OR IGNORE INTO races
            (race_id, race_date, course, race_no, distance_m, surface, going, grade, title)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            f"{race['race_date'].replace('-', '')}{str(races.index(race)).zfill(4)}",
            race['race_date'], race['course'], race['race_no'],
            race['distance_m'], race['surface'], race.get('going'), race.get('grade'), race['title']
        ))
    conn.commit()

    # é¦¬ã¨å‡ºèµ°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    horses = generate_test_horses(count=50)
    for horse in horses:
        conn.execute("""
            INSERT OR IGNORE INTO horses (horse_id, raw_name, sex, birth_year)
            VALUES (?, ?, ?, ?)
        """, (horse['horse_id'], horse['raw_name'], horse['sex'], horse['birth_year']))
    conn.commit()

    # ç‰¹å¾´é‡ã‚’æŠ½å‡º
    print("\nğŸ”§ ç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...")
    X_list = []
    for horse in horses[:30]:
        try:
            features = feat_module.extract_features_for_horse(horse)
            X_list.append(list(features.values()))
        except Exception as e:
            print(f"  ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    if not X_list:
        print("âŒ ç‰¹å¾´é‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    X = np.array(X_list)
    feature_names = feat_module.get_feature_names()

    print(f"âœ… {len(X)} ã‚µãƒ³ãƒ—ãƒ« Ã— {X.shape[1]} ç‰¹å¾´é‡ã‚’æŠ½å‡º")

    # VIF åˆ†æ
    print("\nğŸ“ˆ VIFåˆ†æã‚’å®Ÿè¡Œä¸­...")
    diag = FeatureDiagnostics()
    vif_results = diag.calculate_vif(X, feature_names)
    print("\nğŸ”´ VIF ãŒé«˜ã„ç‰¹å¾´é‡ï¼ˆVIF > 5ï¼‰:")
    high_vif = vif_results[vif_results['VIF'] > 5]
    if len(high_vif) > 0:
        for _, row in high_vif.iterrows():
            print(f"  - {row['Feature']}: {row['VIF']:.2f}")
    else:
        print("  ãªã—ï¼ˆè‰¯å¥½ï¼‰")

    # ç›¸é–¢åˆ†æ
    print("\nğŸ”— ç›¸é–¢åˆ†æã‚’å®Ÿè¡Œä¸­...")
    corr_pairs = diag.find_highly_correlated_pairs(X, feature_names, threshold=0.8)
    if corr_pairs:
        print(f"  é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆr > 0.8ï¼‰: {len(corr_pairs)} ä»¶")
        for pair in corr_pairs[:5]:
            print(f"  - {pair['Feature 1']} â†” {pair['Feature 2']}: {pair['Correlation']:.3f}")
    else:
        print("  ãªã—ï¼ˆè‰¯å¥½ï¼‰")

    # åˆ†æ•£åˆ†æ
    print("\nğŸ“Š åˆ†æ•£åˆ†æã‚’å®Ÿè¡Œä¸­...")
    variance_results = diag.check_feature_variance(X, feature_names)
    print("  åˆ†æ•£ãŒæ¥µç«¯ã«å°ã•ã„ç‰¹å¾´é‡:")
    low_var = variance_results[variance_results['Variance'] < 0.01]
    if len(low_var) > 0:
        for _, row in low_var.iterrows():
            print(f"  - {row['Feature']}: var={row['Variance']:.6f}")
    else:
        print("  ãªã—ï¼ˆè‰¯å¥½ï¼‰")

    print("\nâœ… ç‰¹å¾´é‡è¨ºæ–­å®Œäº†")
    return X, feature_names


def test_learning_curve_diagnostics():
    """Learning Curve ã¨éå­¦ç¿’è¨ºæ–­ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ§ª ãƒ†ã‚¹ãƒˆ 2: Learning Curve ã¨éå­¦ç¿’è¨ºæ–­")
    print("=" * 80)

    # ãƒ€ãƒŸãƒ¼ã®æå¤±ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆè¨“ç·´ã¨æ¤œè¨¼ï¼‰
    print("\nğŸ“Š ãƒ€ãƒŸãƒ¼è¨“ç·´å±¥æ­´ã‚’ç”Ÿæˆä¸­...")
    num_rounds = 100

    # æ­£å¸¸ãªå­¦ç¿’æ›²ç·š
    train_losses = [0.9 - 0.008 * i + np.random.normal(0, 0.01) for i in range(num_rounds)]
    val_losses = [0.9 - 0.007 * i + np.random.normal(0, 0.015) for i in range(num_rounds)]

    print(f"âœ… {num_rounds} ãƒ©ã‚¦ãƒ³ãƒ‰ã®è¨“ç·´å±¥æ­´ã‚’ç”Ÿæˆ")

    # éå­¦ç¿’è¨ºæ–­
    print("\nğŸ” Learning Curve åˆ†æã‚’å®Ÿè¡Œä¸­...")
    diagnosis = ModelTrainingEnhanced.analyze_learning_curve(train_losses, val_losses)

    print(f"\nçµæœ:")
    print(f"  æœ€çµ‚è¨“ç·´æå¤±: {diagnosis['final_train_loss']:.4f}")
    print(f"  æœ€çµ‚æ¤œè¨¼æå¤±: {diagnosis['final_val_loss']:.4f}")
    print(f"  æ±åŒ–ã‚®ãƒ£ãƒƒãƒ—: {diagnosis['generalization_gap']:.4f}")
    print(f"  çŠ¶æ…‹: {diagnosis['status']}")
    print(f"  æ¨å¥¨: {diagnosis['recommendation']}")

    print("\nâœ… Learning Curve åˆ†æå®Œäº†")


def test_fold_metrics():
    """Fold ã”ã¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ§ª ãƒ†ã‚¹ãƒˆ 3: Fold ã”ã¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—")
    print("=" * 80)

    # ãƒ€ãƒŸãƒ¼ã®ãƒ†ã‚¹ãƒˆçµæœã‚’ç”Ÿæˆ
    print("\nğŸ“Š ãƒ€ãƒŸãƒ¼ãƒ†ã‚¹ãƒˆçµæœã‚’ç”Ÿæˆä¸­...")
    n_folds = 5
    y_true_list = []
    y_pred_list = []
    y_pred_proba_list = []

    for fold_idx in range(n_folds):
        n_samples = 100
        y_true = np.random.randint(0, 3, n_samples)
        y_pred = np.where(
            np.random.random(n_samples) > 0.3,
            y_true,
            np.random.randint(0, 3, n_samples)
        )
        y_proba = np.random.dirichlet([1, 1, 1], n_samples)

        y_true_list.append(y_true)
        y_pred_list.append(y_pred)
        y_pred_proba_list.append(y_proba)

    print(f"âœ… {n_folds} Fold ã®ãƒ†ã‚¹ãƒˆçµæœã‚’ç”Ÿæˆ")

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    print("\nğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ä¸­...")
    metrics = ModelTrainingEnhanced.compute_fold_wise_metrics(
        y_true_list, y_pred_list, y_pred_proba_list
    )

    print(f"\nçµæœ:")
    print(f"  å¹³å‡ç²¾åº¦: {metrics['mean_accuracy']:.4f} Â± {metrics['std_accuracy']:.4f}")
    print(f"  å¹³å‡ F1: {metrics['mean_f1_macro']:.4f} Â± {metrics['std_f1_macro']:.4f}")
    print(f"  å¹³å‡ AUC: {metrics.get('mean_auc', np.nan):.4f}")

    print("\nFold è©³ç´°:")
    print(metrics['fold_metrics'].to_string(index=False))

    print("\nâœ… Fold ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("\n" + "ğŸš€" * 40)
    print("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆæ”¹å–„ã®çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("ğŸš€" * 40)

    try:
        # ãƒ†ã‚¹ãƒˆ 1: ç‰¹å¾´é‡è¨ºæ–­
        test_feature_diagnostics()

        # ãƒ†ã‚¹ãƒˆ 2: Learning Curve è¨ºæ–­
        test_learning_curve_diagnostics()

        # ãƒ†ã‚¹ãƒˆ 3: Fold ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        test_fold_metrics()

        print("\n" + "=" * 80)
        print("âœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
