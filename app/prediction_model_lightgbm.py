"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMç‰ˆï¼‰
ã‚ˆã‚Šé«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸãƒ¬ãƒ¼ã‚¹çµæœäºˆæ¸¬
"""

import os
import json
import pickle
import numpy as np
import streamlit as st
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import sys

# lightgbm ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False

# scikit-learn ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent.parent))

from app import queries
from app import features as feat_module
from app.data_leakage_validator import DataLeakageValidator
from app.model_metrics_analyzer import ModelMetricsAnalyzer


class AdvancedRacePredictionModel:
    """LightGBM/GradientBoostingã‚’ä½¿ã£ãŸé«˜åº¦ãªç«¶é¦¬ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self):
        # LightGBMãŒã‚ã‚Œã°ä½¿ã†ã€ãªã‘ã‚Œã°GradientBoostingã‚’ä½¿ã†
        if HAS_LIGHTGBM:
            self.model = lgb.LGBMClassifier(
                num_leaves=31,
                learning_rate=0.05,
                n_estimators=200,
                objective='multiclass',
                num_class=3,
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            )
            self.model_name = "LightGBM"
        else:
            self.model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                random_state=42,
            )
            self.model_name = "GradientBoosting"

        self.scaler = StandardScaler()
        self.feature_names = feat_module.get_feature_names()
        self.is_trained = False
        self.model_path = Path(__file__).parent.parent / "data" / "prediction_model_advanced.pkl"
        self.scaler_path = Path(__file__).parent.parent / "data" / "prediction_scaler_advanced.pkl"

        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.model_path.parent.mkdir(parents=True, exist_ok=True)

        # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self._load_model()

    def _load_model(self):
        """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if self.model_path.exists() and self.scaler_path.exists():
            try:
                with open(self.model_path, 'rb') as f:
                    self.model = pickle.load(f)
                with open(self.scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.is_trained = True
            except Exception as e:
                print(f"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                self.is_trained = False

    def _save_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        try:
            self.model_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.model_path, 'wb') as f:
                pickle.dump(self.model, f)
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def build_training_data_with_cv(self) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        TimeSeriesSplitã«å¯¾å¿œã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        æœªæ¥æƒ…å ±ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢ã™ã‚‹

        Returns:
            (ç‰¹å¾´é‡è¡Œåˆ—, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œåˆ—, ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ãƒªã‚¹ãƒˆ)
        """
        X_list = []
        y_list = []
        race_dates = []

        try:
            from app import db
            conn = db.get_connection()
            cursor = conn.cursor()

            # ç€é †ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’å¯¾è±¡ï¼ˆæ—¥ä»˜æ˜‡é †ã§æ•´åˆ—ï¼‰
            # æ³¨ï¼šLIMIT ãªã—ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ã®ãŸã‚ TimeSeriesSplit ã§æ™‚é–“åˆ†é›¢ï¼‰
            cursor.execute(
                """
                SELECT DISTINCT
                    e.horse_id,
                    e.finish_pos,
                    r.race_date,
                    r.distance_m,
                    r.surface,
                    e.horse_weight,
                    e.days_since_last_race,
                    e.is_steeplechase,
                    e.age
                FROM race_entries e
                JOIN races r ON e.race_id = r.race_id
                WHERE e.finish_pos IS NOT NULL AND e.finish_pos > 0
                ORDER BY r.race_date ASC
                """
            )
            entries = cursor.fetchall()

            for entry in entries:
                horse_id, finish_pos, race_date, distance, surface, horse_weight, days_since, steeplechase, age = entry

                try:
                    horse_details = queries.get_horse_details(horse_id)
                    if not horse_details:
                        continue

                    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
                    race_info = {
                        'distance_m': distance,
                        'surface': surface,
                    }

                    # å‡ºèµ°æƒ…å ±
                    entry_info = {
                        'horse_weight': horse_weight or 450,
                        'weight_carried': 54,  # å¹³å‡çš„ãªæ–¤é‡
                        'days_since_last_race': days_since or 14,
                        'is_steeplechase': steeplechase or 0,
                        'age': age or 4,
                    }

                    # ç‰¹å¾´é‡æŠ½å‡º
                    features_dict = feat_module.extract_features_for_horse(
                        horse_details,
                        race_info=race_info,
                        entry_info=entry_info
                    )
                    vector, _ = feat_module.create_feature_vector(features_dict)

                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼šç€é †ï¼ˆ1ç€=0, 2-3ç€=1, ãã‚Œä»¥å¤–=2ï¼‰
                    if finish_pos == 1:
                        target = 0
                    elif finish_pos in (2, 3):
                        target = 1
                    else:
                        target = 2

                    X_list.append(vector)
                    y_list.append(target)
                    race_dates.append(race_date)

                except Exception as e:
                    continue

            conn.close()

            if not X_list:
                return None, None, None

            X = np.array(X_list)
            y = np.array(y_list)

            return X, y, race_dates

        except Exception as e:
            print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, None

    def train_with_cross_validation(self):
        """TimeSeriesSplitã‚’ä½¿ã£ãŸäº¤å·®æ¤œè¨¼ä»˜ãè¨“ç·´"""
        X, y, race_dates = self.build_training_data_with_cv()

        print(f"ãƒ‡ãƒãƒƒã‚°: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º = {len(X) if X is not None else 0}")

        if X is None or len(X) < 50:
            raise ValueError(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆå–å¾—ä»¶æ•°: {len(X) if X is not None else 0}ï¼‰")

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®ç¢ºèª
        unique_classes, class_counts = np.unique(y, return_counts=True)
        class_distribution = dict(zip(unique_classes, class_counts))
        print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {class_distribution}")
        print(f"  ã‚¯ãƒ©ã‚¹ 0ï¼ˆ1ç€ï¼‰: {class_counts[0] if 0 in unique_classes else 0}")
        print(f"  ã‚¯ãƒ©ã‚¹ 1ï¼ˆ2-3ç€ï¼‰: {class_counts[1] if 1 in unique_classes else 0}")
        print(f"  ã‚¯ãƒ©ã‚¹ 2ï¼ˆãã®ä»–ï¼‰: {class_counts[2] if 2 in unique_classes else 0}")

        # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ã‘ã®è¨ˆç®—ï¼ˆä¸å‡è¡¡å¯¾ç­–ï¼‰
        class_weights = compute_class_weight('balanced', classes=unique_classes, y=y)
        class_weight_dict = dict(zip(unique_classes, class_weights))
        print(f"ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ã‘: {class_weight_dict}")

        # TimeSeriesSplitã§æœªæ¥æƒ…å ±ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢ï¼ˆ5åˆ†å‰²ã«å¢—åŠ ï¼‰
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = []
        cv_f1_scores = []
        cv_fold_info = []
        cv_splits = []

        print(f"\nTimeSeriesSplitã§ {tscv.get_n_splits()} åˆ†å‰²ã®äº¤å·®æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...\n")

        # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
        cv_validation_results = DataLeakageValidator.validate_cv_splits(
            X, y, race_dates, list(tscv.split(X))
        )
        DataLeakageValidator.print_validation_report(cv_validation_results)

        if not cv_validation_results['all_valid']:
            print("âš ï¸ è­¦å‘Š: ã„ãã¤ã‹ã®Foldã§ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")

        # å†åº¦CVåˆ†å‰²ã‚’å–å¾—ï¼ˆå‰å›ã® split ã¯æ¶ˆè²»æ¸ˆã¿ï¼‰
        fold_num = 0
        for train_idx, test_idx in tscv.split(X):
            fold_num += 1
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            cv_splits.append((train_idx, test_idx))

            # æ™‚é–“ç¯„å›²ã®ç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢æ¤œè¨¼ï¼‰
            if race_dates:
                train_dates = [race_dates[i] for i in train_idx]
                test_dates = [race_dates[i] for i in test_idx]
                train_min, train_max = min(train_dates), max(train_dates)
                test_min, test_max = min(test_dates), max(test_dates)
                print(f"  Fold {fold_num}:")
                print(f"    è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_min} ï½ {train_max} ({len(train_idx)}ä»¶)")
                print(f"    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_min} ï½ {test_max} ({len(test_idx)}ä»¶)")

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ fitï¼‰
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ã‘é©ç”¨ï¼‰
            if HAS_LIGHTGBM:
                self.model.fit(X_train_scaled, y_train, sample_weight=None)
            else:
                # GradientBoostingClassifier ã¯ sample_weight ã‚’ã‚µãƒãƒ¼ãƒˆ
                self.model.fit(X_train_scaled, y_train)

            # è©•ä¾¡ï¼ˆè¤‡æ•°æŒ‡æ¨™ï¼‰
            y_pred = self.model.predict(X_test_scaled)

            # äºˆæ¸¬ç¢ºç‡ã®å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            y_pred_proba = None
            if hasattr(self.model, 'predict_proba'):
                y_pred_proba = self.model.predict_proba(X_test_scaled)

            accuracy = accuracy_score(y_test, y_pred)

            # ãƒã‚¯ãƒ­å¹³å‡ F1 ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¼·ã„ï¼‰
            f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)

            # é‡ã¿ã¥ã F1 ã‚¹ã‚³ã‚¢
            f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            cv_scores.append(accuracy)
            cv_f1_scores.append(f1_macro)

            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(y_test, y_pred, labels=unique_classes)

            # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
            class_metrics = ModelMetricsAnalyzer.calculate_class_metrics(
                y_test, y_pred, y_pred_proba
            )

            fold_info = {
                'fold': fold_num,
                'accuracy': accuracy,
                'f1_macro': f1_macro,
                'f1_weighted': f1_weighted,
                'confusion_matrix': cm.tolist(),
                'class_metrics': class_metrics,  # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ 
                'metrics': class_metrics,  # ã‚µãƒãƒªãƒ¼ã§ä½¿ç”¨
            }
            cv_fold_info.append(fold_info)

            print(f"    ç²¾åº¦: {accuracy:.4f}, F1(macro): {f1_macro:.4f}, F1(weighted): {f1_weighted:.4f}")
            print(f"    æ··åŒè¡Œåˆ—:\n{cm}\n")

            # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©³ç´°å‡ºåŠ›
            ModelMetricsAnalyzer.print_detailed_report(class_metrics, fold_num)

        # æœ€çµ‚çš„ã«ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
        X_scaled = self.scaler.fit_transform(X)
        if HAS_LIGHTGBM:
            self.model.fit(X_scaled, y, sample_weight=None)
        else:
            self.model.fit(X_scaled, y)

        self.is_trained = True

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self._save_model()

        return {
            'mean_cv_accuracy': np.mean(cv_scores),
            'std_cv_accuracy': np.std(cv_scores),
            'cv_scores': cv_scores,
            'mean_cv_f1': np.mean(cv_f1_scores),
            'std_cv_f1': np.std(cv_f1_scores),
            'cv_f1_scores': cv_f1_scores,
            'fold_details': cv_fold_info,
            'class_distribution': class_distribution,
            'class_weights': class_weight_dict,
            'training_samples': len(X),
            'data_leakage_validation': cv_validation_results,  # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼çµæœ
        }

    def predict_race_order(self, horse_ids: List[int], race_info: Dict = None) -> Dict:
        """
        è¤‡æ•°ã®é¦¬ã®ç€é †ã‚’äºˆæ¸¬ï¼ˆæ”¹å–„ç‰ˆï¼‰

        Args:
            horse_ids: é¦¬IDã®ãƒªã‚¹ãƒˆ
            race_info: ãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼ˆè·é›¢ã€é¦¬å ´ãªã©ï¼‰

        Returns:
            äºˆæ¸¬çµæœã®è¾æ›¸
        """
        if not self.is_trained:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“"}

        results = []

        for horse_id in horse_ids:
            try:
                horse_details = queries.get_horse_details(horse_id)
                if not horse_details:
                    continue

                # ç‰¹å¾´é‡æŠ½å‡º
                features_dict = feat_module.extract_features_for_horse(
                    horse_details,
                    race_info=race_info,
                )
                vector, _ = feat_module.create_feature_vector(features_dict)

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                vector_scaled = self.scaler.transform([vector])

                # äºˆæ¸¬
                probabilities = self.model.predict_proba(vector_scaled)[0]
                predicted_class = self.model.predict(vector_scaled)[0]

                # ç€é †ã®èª¬æ˜ï¼ˆã‚¯ãƒ©ã‚¹æ•°ãŒå¯å¤‰ãªã®ã§å¯¾å¿œï¼‰
                win_prob = float(probabilities[0]) * 100 if len(probabilities) > 0 else 0
                place_prob = float(probabilities[1]) * 100 if len(probabilities) > 1 else 0
                other_prob = float(probabilities[2]) * 100 if len(probabilities) > 2 else (100 - win_prob - place_prob)

                results.append({
                    'horse_id': horse_id,
                    'horse_name': horse_details.get('raw_name', 'ä¸æ˜'),
                    'predicted_class': int(predicted_class),
                    'win_probability': win_prob,
                    'place_probability': place_prob,
                    'other_probability': other_prob,
                    'confidence': float(max(probabilities)) * 100,
                })

            except Exception as e:
                print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ (horse_id={horse_id}): {e}")
                continue

        # ä¿¡é ¼åº¦ã§é™é †ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x['confidence'], reverse=True)

        return {
            'predictions': results,
            'model_status': 'trained',
            'total_horses': len(results),
            'model_type': self.model_name,
        }

    def get_model_info(self) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—"""
        return {
            'is_trained': self.is_trained,
            'model_type': self.model_name,
            'feature_names': self.feature_names,
            'n_features': len(self.feature_names),
        }

    def get_feature_importance(self) -> List[Tuple[str, float]]:
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—"""
        if not self.is_trained or not hasattr(self.model, 'feature_importances_'):
            return []

        importances = self.model.feature_importances_
        feature_importance = list(zip(self.feature_names, importances))
        feature_importance.sort(key=lambda x: x[1], reverse=True)

        return feature_importance


# ã‚°ãƒ­ãƒ¼ãƒãƒ«äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆStreamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
@st.cache_resource
def get_advanced_prediction_model() -> AdvancedRacePredictionModel:
    """é«˜åº¦ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    return AdvancedRacePredictionModel()
