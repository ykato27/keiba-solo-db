"""
ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ã‚¯ãƒ©ã‚¹åˆ¥ã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã®çœŸã®æ€§èƒ½ã‚’è©•ä¾¡

æ©Ÿèƒ½:
1. ã‚¯ãƒ©ã‚¹åˆ¥ Precision, Recall, F1ã‚¹ã‚³ã‚¢
2. æ··åŒè¡Œåˆ—ã®è©³ç´°åˆ†æ
3. ç€é †äºˆæ¸¬ã®ç‰¹æ€§è©•ä¾¡ï¼ˆ1ç€, è¤‡å‹, ãã®ä»–ã”ã¨ï¼‰
4. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ
5. äºˆæ¸¬ä¿¡é ¼åº¦ã®åˆ†å¸ƒ
"""

import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from sklearn.metrics import (
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    log_loss,
)


class ModelMetricsAnalyzer:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã®å®šç¾©
    CLASS_LABELS = {0: "1ç€", 1: "è¤‡å‹(2-3ç€)", 2: "ãã®ä»–"}

    @staticmethod
    def calculate_class_metrics(
        y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """
        ã‚¯ãƒ©ã‚¹åˆ¥ã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—

        Args:
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_pred_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸
        """
        result = {
            "global_metrics": {},
            "class_metrics": {},
            "confusion_matrix": None,
            "class_distribution": {},
        }

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        unique_classes = np.unique(y_true)

        # ãƒã‚¯ãƒ­å¹³å‡ã¨é‡ã¿å¹³å‡ã® Precision, Recall, F1
        p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(
            y_true, y_pred, average="macro", zero_division=0
        )
        p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(
            y_true, y_pred, average="weighted", zero_division=0
        )

        result["global_metrics"] = {
            "accuracy": float(np.mean(y_pred == y_true)),
            "precision_macro": float(p_macro),
            "recall_macro": float(r_macro),
            "f1_macro": float(f1_macro),
            "precision_weighted": float(p_weighted),
            "recall_weighted": float(r_weighted),
            "f1_weighted": float(f1_weighted),
        }

        # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, labels=unique_classes, zero_division=0
        )

        for i, cls in enumerate(unique_classes):
            class_name = ModelMetricsAnalyzer.CLASS_LABELS.get(int(cls), f"Class {cls}")
            result["class_metrics"][int(cls)] = {
                "class_name": class_name,
                "precision": float(precision[i]),
                "recall": float(recall[i]),
                "f1_score": float(f1[i]),
                "support": int(support[i]),
            }

        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(y_true, y_pred, labels=unique_classes)
        result["confusion_matrix"] = {
            "matrix": cm.tolist(),
            "labels": [int(c) for c in unique_classes],
            "normalized_by_true": cm.astype("float") / cm.sum(axis=1, keepdims=True),
            "normalized_by_pred": cm.astype("float") / cm.sum(axis=0, keepdims=True),
        }

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        for cls in unique_classes:
            count = np.sum(y_true == cls)
            pct = count / len(y_true) * 100
            result["class_distribution"][int(cls)] = {
                "count": int(count),
                "percentage": float(pct),
                "class_name": ModelMetricsAnalyzer.CLASS_LABELS.get(int(cls), f"Class {cls}"),
            }

        # äºˆæ¸¬ç¢ºç‡ãŒã‚ã‚‹å ´åˆã€è¿½åŠ ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if y_pred_proba is not None:
            result["probability_metrics"] = ModelMetricsAnalyzer._analyze_probabilities(
                y_true, y_pred_proba, unique_classes
            )

        return result

    @staticmethod
    def _analyze_probabilities(
        y_true: np.ndarray, y_pred_proba: np.ndarray, unique_classes: np.ndarray
    ) -> Dict[str, Any]:
        """äºˆæ¸¬ç¢ºç‡ã®åˆ†æ"""
        prob_result = {
            "mean_confidence": float(np.max(y_pred_proba, axis=1).mean()),
            "median_confidence": float(np.median(np.max(y_pred_proba, axis=1))),
            "confidence_by_class": {},
            "log_loss": 0.0,
        }

        # ã‚¯ãƒ©ã‚¹åˆ¥ã®å¹³å‡ä¿¡é ¼åº¦
        for cls in unique_classes:
            mask = y_true == cls
            if np.sum(mask) > 0:
                max_probs_for_class = np.max(y_pred_proba[mask], axis=1)
                prob_result["confidence_by_class"][int(cls)] = {
                    "mean": float(max_probs_for_class.mean()),
                    "std": float(max_probs_for_class.std()),
                    "min": float(max_probs_for_class.min()),
                    "max": float(max_probs_for_class.max()),
                }

        # ãƒ­ã‚°ãƒ­ã‚¹ï¼ˆãƒã‚¤ãƒ†ã‚£ãƒ–ã«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ï¼‰
        try:
            prob_result["log_loss"] = float(log_loss(y_true, y_pred_proba))
        except:
            prob_result["log_loss"] = None

        return prob_result

    @staticmethod
    def calculate_per_fold_metrics(fold_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è¤‡æ•°ã®Foldã®çµæœã‹ã‚‰çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—

        Args:
            fold_results: å„Foldã®çµæœãƒªã‚¹ãƒˆ

        Returns:
            çµ±è¨ˆã‚µãƒãƒªãƒ¼
        """
        summary = {
            "total_folds": len(fold_results),
            "accuracy_mean": 0.0,
            "accuracy_std": 0.0,
            "accuracy_min": 0.0,
            "accuracy_max": 0.0,
            "f1_macro_mean": 0.0,
            "f1_macro_std": 0.0,
            "f1_weighted_mean": 0.0,
            "f1_weighted_std": 0.0,
            "class_specific_stats": {},
        }

        accuracies = []
        f1_macros = []
        f1_weighteds = []

        for fold_result in fold_results:
            if "metrics" in fold_result:
                metrics = fold_result["metrics"]
                accuracies.append(metrics.get("global_metrics", {}).get("accuracy", 0))
                f1_macros.append(metrics.get("global_metrics", {}).get("f1_macro", 0))
                f1_weighteds.append(metrics.get("global_metrics", {}).get("f1_weighted", 0))

        if accuracies:
            summary["accuracy_mean"] = float(np.mean(accuracies))
            summary["accuracy_std"] = float(np.std(accuracies))
            summary["accuracy_min"] = float(np.min(accuracies))
            summary["accuracy_max"] = float(np.max(accuracies))

        if f1_macros:
            summary["f1_macro_mean"] = float(np.mean(f1_macros))
            summary["f1_macro_std"] = float(np.std(f1_macros))

        if f1_weighteds:
            summary["f1_weighted_mean"] = float(np.mean(f1_weighteds))
            summary["f1_weighted_std"] = float(np.std(f1_weighteds))

        return summary

    @staticmethod
    def print_detailed_report(metrics_dict: Dict[str, Any], fold_num: Optional[int] = None) -> None:
        """è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›"""
        header = f"Fold {fold_num}" if fold_num else "å…¨ä½“è©•ä¾¡"
        print("\n" + "=" * 80)
        print(f"ğŸ“Š {header} - è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        print("=" * 80)

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if "global_metrics" in metrics_dict:
            print("\nã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘")
            gm = metrics_dict["global_metrics"]
            print(f"  ç²¾åº¦ (Accuracy):         {gm.get('accuracy', 0):.4f}")
            print(f"  Precision (ãƒã‚¯ãƒ­å¹³å‡): {gm.get('precision_macro', 0):.4f}")
            print(f"  Recall (ãƒã‚¯ãƒ­å¹³å‡):    {gm.get('recall_macro', 0):.4f}")
            print(f"  F1 Score (ãƒã‚¯ãƒ­å¹³å‡):  {gm.get('f1_macro', 0):.4f}")
            print(f"  Precision (é‡ã¿å¹³å‡):   {gm.get('precision_weighted', 0):.4f}")
            print(f"  Recall (é‡ã¿å¹³å‡):      {gm.get('recall_weighted', 0):.4f}")
            print(f"  F1 Score (é‡ã¿å¹³å‡):    {gm.get('f1_weighted', 0):.4f}")

        # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if "class_metrics" in metrics_dict:
            print("\nã€ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘")
            for class_id, metrics in metrics_dict["class_metrics"].items():
                class_name = metrics.get("class_name", f"Class {class_id}")
                print(f"\n  {class_name}:")
                print(f"    Precision: {metrics.get('precision', 0):.4f}")
                print(f"    Recall:    {metrics.get('recall', 0):.4f}")
                print(f"    F1 Score:  {metrics.get('f1_score', 0):.4f}")
                print(f"    Support:   {metrics.get('support', 0)}")

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        if "class_distribution" in metrics_dict:
            print("\nã€ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã€‘")
            for class_id, dist in metrics_dict["class_distribution"].items():
                class_name = dist.get("class_name", f"Class {class_id}")
                print(f"  {class_name}: {dist.get('count', 0)} ({dist.get('percentage', 0):.1f}%)")

        # æ··åŒè¡Œåˆ—
        if "confusion_matrix" in metrics_dict:
            print("\nã€æ··åŒè¡Œåˆ—ã€‘")
            cm = np.array(metrics_dict["confusion_matrix"]["matrix"])
            print(cm)

        # äºˆæ¸¬ç¢ºç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if "probability_metrics" in metrics_dict:
            print("\nã€äºˆæ¸¬ç¢ºç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘")
            pm = metrics_dict["probability_metrics"]
            print(f"  å¹³å‡ä¿¡é ¼åº¦: {pm.get('mean_confidence', 0):.4f}")
            print(f"  ä¸­å¤®å€¤ä¿¡é ¼åº¦: {pm.get('median_confidence', 0):.4f}")
            if pm.get("log_loss"):
                print(f"  Log Loss: {pm.get('log_loss', 0):.4f}")

        print("\n" + "=" * 80 + "\n")

    @staticmethod
    def get_model_strength_assessment(metrics_dict: Dict[str, Any]) -> Dict[str, str]:
        """
        ãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ãƒ»å¼±ã¿ã‚’è©•ä¾¡

        Returns:
            å¼·ã¿ãƒ»å¼±ã¿ã®è©•ä¾¡
        """
        assessment = {
            "strengths": [],
            "weaknesses": [],
            "recommendations": [],
        }

        if "global_metrics" not in metrics_dict:
            return assessment

        gm = metrics_dict["global_metrics"]
        f1_macro = gm.get("f1_macro", 0)

        # F1ã‚¹ã‚³ã‚¢ã®è©•ä¾¡
        if f1_macro >= 0.7:
            assessment["strengths"].append("âœ… å…¨ä½“çš„ã«è‰¯å¥½ãªäºˆæ¸¬ç²¾åº¦ï¼ˆF1 >= 0.7ï¼‰")
        elif f1_macro >= 0.5:
            assessment["strengths"].append("âš ï¸ ä¸­ç¨‹åº¦ã®äºˆæ¸¬ç²¾åº¦ï¼ˆF1 >= 0.5ï¼‰")
        else:
            assessment["weaknesses"].append(
                "âŒ ä½ã„äºˆæ¸¬ç²¾åº¦ï¼ˆF1 < 0.5ï¼‰ã€‚ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„ãŒå¿…è¦"
            )
            assessment["recommendations"].append("ç‰¹å¾´é‡ã‚’è¿½åŠ ãƒ»æ”¹è‰¯ã™ã‚‹ï¼ˆé¨æ‰‹ã€èª¿æ•™å¸«æƒ…å ±ãªã©ï¼‰")

        # ã‚¯ãƒ©ã‚¹åˆ¥æ€§èƒ½ã®ä¸å‡è¡¡
        if "class_metrics" in metrics_dict:
            f1_scores = [m.get("f1_score", 0) for m in metrics_dict["class_metrics"].values()]
            if max(f1_scores) - min(f1_scores) > 0.2:
                assessment["weaknesses"].append(
                    f"ã‚¯ãƒ©ã‚¹åˆ¥æ€§èƒ½ãŒä¸å‡è¡¡ï¼ˆF1ã®å·®: {max(f1_scores) - min(f1_scores):.2f}ï¼‰"
                )
                assessment["recommendations"].append(
                    "ã‚¯ãƒ©ã‚¹ã®é‡ã¿ä»˜ã‘ã‚’èª¿æ•´ã™ã‚‹ã‹ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°/ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æ¤œè¨"
                )

        # RecallãŒä½ã„å ´åˆ
        recall_macro = gm.get("recall_macro", 0)
        if recall_macro < 0.5:
            assessment["weaknesses"].append("âŒ Recallï¼ˆå†ç¾ç‡ï¼‰ãŒä½ã„ã€‚äºˆæ¸¬æ¼ã‚ŒãŒå¤šã„å¯èƒ½æ€§")
            assessment["recommendations"].append(
                "ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã§è©¦ã™ã€ã¾ãŸã¯ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ã‘ã‚’èª¿æ•´"
            )

        return assessment
